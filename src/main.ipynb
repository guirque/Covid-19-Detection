{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UF8rKAX1Fb5"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xXEJUHYM1IYe"
      },
      "outputs": [],
      "source": [
        "import setup\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQTR_8N6xp-F"
      },
      "source": [
        "# Dataset Downloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJUdeIXxzRp8"
      },
      "source": [
        "Covid-19 Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HMZw3_hvzY1",
        "outputId": "0858e4c4-7cfa-45bd-b255-feb5d98539d1"
      },
      "outputs": [],
      "source": [
        "#!curl -L -o /content/sample_data/covid19-image-dataset.zip\\\n",
        "#  https://www.kaggle.com/api/v1/datasets/download/pranavraikokte/covid19-image-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4OlO8Ie1yUfg"
      },
      "outputs": [],
      "source": [
        "#from zipfile import ZipFile\n",
        "#with ZipFile('/content/sample_data/covid19-image-dataset.zip') as zp:\n",
        "#  zp.extractall('/content/sample_data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEcpmFJuzXII"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Largest: width 4248 and height 4095\n",
            "Mean values: width 1717.9721115537848 and height 1439.98406374502\n"
          ]
        }
      ],
      "source": [
        "# Checking Biggest Image Sizes\n",
        "from glob import glob\n",
        "from setup import DATA_PATH\n",
        "import os\n",
        "from PIL import Image\n",
        "imgs = glob(os.path.join(DATA_PATH, 'train', '*', '*.jpg'), recursive=True) + glob(os.path.join(DATA_PATH, 'train', '*', '*.png'), recursive=True) + glob(os.path.join(DATA_PATH, 'train', '*', '*.jpeg'), recursive=True)\n",
        "\n",
        "widths = []\n",
        "heights = []\n",
        "\n",
        "for img in imgs:\n",
        "    pil_image = Image.open(img)\n",
        "    widths.append(pil_image.width)\n",
        "    heights.append(pil_image.height)\n",
        "\n",
        "print(f'Largest: width {max(widths)} and height {max(heights)}')\n",
        "print(f'Mean values: width {sum(widths)/len(widths)} and height {sum(heights)/len(heights)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "SC6mfpKLznRZ"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from setup import IMAGE_WIDTH, IMAGE_HEIGHT\n",
        "from data.loading import load_data\n",
        "from math import ceil\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(size=(IMAGE_WIDTH, IMAGE_HEIGHT)), transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "train_ds, test_ds = load_data(transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbBdZIjn_JiO"
      },
      "source": [
        "We might not have to create a custom dataset, because we can use ImageFolder to create a dataset from an image folder with an organized structure. It will automatically associate the class names to images according to folder names.\n",
        "\n",
        "https://debuggercafe.com/pytorch-imagefolder-for-training-cnn-models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljsUITP1Dype"
      },
      "source": [
        "Images loaded are of different sizes, which would cause a\n",
        "```\n",
        "RuntimeError: stack expects each tensor to be equal size, but got [3, 3480, 4248] at entry 0 and [3, 1303, 1458] at entry 1\n",
        "\n",
        "```\n",
        "\n",
        "We can fix that by assuring all images are of the same size. Use a transform for that.\n",
        "\n",
        "https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XplHOEmvGGkZ"
      },
      "source": [
        "Apparently, we can normalize the channels to make sure brighter colors won't be of more importance. Use a transform for that.\n",
        "\n",
        "https://stats.stackexchange.com/questions/211436/why-normalize-images-by-subtracting-datasets-image-mean-instead-of-the-current"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXadZTFgSVPC"
      },
      "source": [
        "# Analyzing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "8x5YkmOqSaMa",
        "outputId": "8079a354-9a05-447a-a1e9-9a5475f50f8d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "Class",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Img_Uri",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "ref": "7aa88098-7262-404f-9164-3be22db514be",
              "rows": [
                [
                  "0",
                  "111"
                ],
                [
                  "1",
                  "70"
                ],
                [
                  "2",
                  "70"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 3
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Img_Uri</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Class</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Img_Uri\n",
              "Class         \n",
              "0          111\n",
              "1           70\n",
              "2           70"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(train_ds.imgs)\n",
        "df = df.rename(columns={0: 'Img_Uri', 1: 'Class'})\n",
        "classes = df.loc[:, 'Class']\n",
        "\n",
        "plt.hist(classes)\n",
        "plt.xticks([0, 1, 2], labels=train_ds.classes)\n",
        "plt.show()\n",
        "\n",
        "df.groupby(['Class']).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ll7akpnbn7D"
      },
      "source": [
        "# Modifying Data\n",
        "\n",
        "-> Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "251"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqUqQR4qbngG",
        "outputId": "be3a636d-89bd-4496-9569-4df23e1afdbb"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "iteration = 0\n",
        "new_imgs = []\n",
        "targets = []\n",
        "samples = []\n",
        "for img in train_ds.imgs:\n",
        "  if img[1] == 0:\n",
        "    if i >= 70:\n",
        "      continue\n",
        "    i += 1\n",
        "\n",
        "  targets.append(train_ds.targets[iteration])\n",
        "  new_imgs.append(img)\n",
        "  samples.append(train_ds.samples[iteration])\n",
        "  iteration += 1\n",
        "\n",
        "train_ds.imgs = new_imgs\n",
        "train_ds.targets = targets\n",
        "train_ds.samples = samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_IMGS = len(train_ds.imgs)\n",
        "NUM_BATCHES = ceil(NUM_IMGS/setup.BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "210"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from setup import BATCH_SIZE\n",
        "\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "iterable_ds = iter(loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Declaring Components\n",
        "\n",
        "- Model\n",
        "- Optimizer\n",
        "- Loss Function\n",
        "- Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2fMaPRP0t33"
      },
      "source": [
        "## Making Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xPPWB5GjHqH2"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from setup import FINE_TUNE\n",
        "from model.load_model import create_model\n",
        "\n",
        "model = create_model(fine_tune=FINE_TUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZWz0czDQNFc"
      },
      "source": [
        "## Setting Up Optimizer, Loss Function and Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "O6CXjKTpQP1j"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=setup.LEARNING_RATE, momentum=0.9)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Setting Up Scheduler\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=10,\n",
        "    gamma=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Jbwh5FPl_K"
      },
      "source": [
        "# Training\n",
        "\n",
        "Apparently, to run on the GPU, we have to send the data itself to the GPU. We can do so with the batches. The loss function also needs the labels to be loaded on the GPU.\n",
        "\n",
        "https://discuss.pytorch.org/t/how-to-load-all-data-into-gpu-for-training/27609/22?page=2\n",
        "\n",
        "Also, we need the weights to be on the GPU. We can do that by using the model itself, sending it to the GPU as well.\n",
        "\n",
        "https://discuss.pytorch.org/t/how-to-load-all-data-into-gpu-for-training/27609/34?page=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "d8w-uHjaA5w9"
      },
      "outputs": [],
      "source": [
        "# GPU or CPU Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = model.to(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_UsCHhuQFsn"
      },
      "source": [
        "Tqdm is a simple library that can be used to generate a loading bar on loops.\n",
        "\n",
        "https://github.com/tqdm/tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUZNKPmvPhzn",
        "outputId": "63d21b6e-b5e1-424c-938b-47a3537112d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 20/20 [00:01<00:00, 10.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Accuracy: 72.8571%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: 100%|██████████| 20/20 [00:01<00:00, 10.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Accuracy: 82.3810%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2: 100%|██████████| 20/20 [00:01<00:00, 10.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Accuracy: 82.3810%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3: 100%|██████████| 20/20 [00:01<00:00, 10.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Accuracy: 78.0952%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4: 100%|██████████| 20/20 [00:01<00:00, 10.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Accuracy: 81.9048%\n",
            "Switching to model with accuracy of 82.3810%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(82.3810, device='cuda:0')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from model.train import train\n",
        "\n",
        "train(\n",
        "    model=model,\n",
        "    num_batches=NUM_BATCHES,\n",
        "    num_epochs=setup.NUM_EPOCHS,\n",
        "    device=device,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    dataset=train_ds,\n",
        "    loss=loss,\n",
        "    num_imgs=NUM_IMGS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training With Stratified Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array(train_ds.imgs + test_ds.imgs)\n",
        "y = np.array(train_ds.targets + test_ds.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
        "\n",
        "split_sets = skf.split(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0\n",
            "Train Accuracy: 76.3636%\n",
            "Test Accuracy: 92.8571%\n",
            "\n",
            "Fold 1\n",
            "Train Accuracy: 80.5430%\n",
            "Test Accuracy: 65.4545%\n",
            "\n",
            "Fold 2\n",
            "Train Accuracy: 77.8281%\n",
            "Test Accuracy: 70.9091%\n",
            "\n",
            "Fold 3\n",
            "Train Accuracy: 77.8281%\n",
            "Test Accuracy: 76.3636%\n",
            "\n",
            "Fold 4\n",
            "Train Accuracy: 80.0905%\n",
            "Test Accuracy: 74.5455%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import PIL\n",
        "\n",
        "class custom_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y, transform):\n",
        "        super().__init__()\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        file_path = self.X[index][0]\n",
        "        label = self.y[index]\n",
        "\n",
        "        pil_image = PIL.Image.open(file_path)\n",
        "        pil_image = pil_image.convert('RGB')\n",
        "\n",
        "        # Reference: https://stackoverflow.com/questions/76137400/how-to-feed-on-single-image-into-a-pytorch-cnn\n",
        "        model_input = self.transform(pil_image).float().unsqueeze(0)\n",
        "\n",
        "        return model_input[0], label # we return the image data [num_channels, width, height] and the corresponding label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "        \n",
        "\n",
        "# Based on documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(split_sets):\n",
        "\n",
        "    # Loading Stratified Data ======================\n",
        "\n",
        "    ds = custom_dataset(X[train_index], y[train_index], transform) # custom dataset, with images chosen by Stratified K Fold\n",
        "    num_imgs = len(train_index)\n",
        "    \n",
        "    # Creating New Model and Components ============\n",
        "\n",
        "    model = create_model(fine_tune=FINE_TUNE)\n",
        "    model = model.train()\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=setup.LEARNING_RATE, momentum=0.9)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    scheduler = lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=10,\n",
        "        gamma=0.5\n",
        "    )\n",
        "\n",
        "    # Training =====================================\n",
        "    accuracy = train(\n",
        "        model=model,\n",
        "        num_batches=NUM_BATCHES,\n",
        "        num_epochs=setup.NUM_EPOCHS,\n",
        "        device=device,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        dataset=ds,\n",
        "        loss=loss,\n",
        "        num_imgs=num_imgs,\n",
        "        verbose=False\n",
        "    )\n",
        "    print(f'Fold {i}')\n",
        "    print(f'Train Accuracy: {accuracy:.4f}%')\n",
        "\n",
        "    # Testing =======================================\n",
        "\n",
        "    model = model.eval()\n",
        "\n",
        "    accuracy = 0\n",
        "    test_X = X[test_index]\n",
        "    test_y = y[test_index]\n",
        "    ds = custom_dataset(X, y, transform)\n",
        "    num_imgs = len(test_y)\n",
        "\n",
        "    for index in test_index:\n",
        "\n",
        "        model_input, label = ds.__getitem__(index)\n",
        "        model_input = model_input.to(device)\n",
        "\n",
        "        ms = model_input.shape\n",
        "\n",
        "        model_input = model_input.view(1, ms[0], ms[1], ms[2]) # create a single batch with the image data (wrap it in a tensor)\n",
        "        \n",
        "        probabilities = torch.softmax(model(model_input), dim=1)\n",
        "        prediction = torch.argmax(probabilities).item()\n",
        "\n",
        "        if prediction == label:\n",
        "            accuracy += 1\n",
        "    print(f'Test Accuracy: {accuracy*100/num_imgs:.4f}%\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzhkMeogcEKT"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijprcBF46cc1"
      },
      "source": [
        "Evaluating with Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from data.evaluation import evaluate_model\n",
        "\n",
        "#evaluate_model(model=model,\n",
        "#               test_data=test_ds,\n",
        "#               batch_size=BATCH_SIZE\n",
        "#               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing Against Images of Class Covid: 100%|██████████| 26/26 [00:00<00:00, 91.83it/s] \n",
            "Testing Against Images of Class Normal: 100%|██████████| 20/20 [00:00<00:00, 42.49it/s]\n",
            "Testing Against Images of Class Viral Pneumonia: 100%|██████████| 20/20 [00:00<00:00, 75.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 56/66 | 84.8485%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from data.evaluation import evaluate_single\n",
        "\n",
        "evaluate_single(model=model, classes_list=['Covid', 'Normal', 'Viral Pneumonia'], device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model.save_model import save_model\n",
        "\n",
        "save_model(model=model, optimizer=optimizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
