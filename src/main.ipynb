{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UF8rKAX1Fb5"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXEJUHYM1IYe"
      },
      "outputs": [],
      "source": [
        "import setup\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQTR_8N6xp-F"
      },
      "source": [
        "# Dataset Downloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJUdeIXxzRp8"
      },
      "source": [
        "Covid-19 Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HMZw3_hvzY1",
        "outputId": "0858e4c4-7cfa-45bd-b255-feb5d98539d1"
      },
      "outputs": [],
      "source": [
        "#!curl -L -o /content/sample_data/covid19-image-dataset.zip\\\n",
        "#  https://www.kaggle.com/api/v1/datasets/download/pranavraikokte/covid19-image-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OlO8Ie1yUfg"
      },
      "outputs": [],
      "source": [
        "#from zipfile import ZipFile\n",
        "#with ZipFile('/content/sample_data/covid19-image-dataset.zip') as zp:\n",
        "#  zp.extractall('/content/sample_data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEcpmFJuzXII"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking Biggest Image Sizes\n",
        "from glob import glob\n",
        "from setup import DATA_PATH\n",
        "import os\n",
        "from PIL import Image\n",
        "imgs = glob(os.path.join(DATA_PATH, 'train', '*', '*.jpg'), recursive=True) + glob(os.path.join(DATA_PATH, 'train', '*', '*.png'), recursive=True) + glob(os.path.join(DATA_PATH, 'train', '*', '*.jpeg'), recursive=True)\n",
        "\n",
        "widths = []\n",
        "heights = []\n",
        "\n",
        "for img in imgs:\n",
        "    pil_image = Image.open(img)\n",
        "    widths.append(pil_image.width)\n",
        "    heights.append(pil_image.height)\n",
        "\n",
        "print(f'Largest: width {max(widths)} and height {max(heights)}')\n",
        "print(f'Mean values: width {sum(widths)/len(widths)} and height {sum(heights)/len(heights)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SC6mfpKLznRZ"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from setup import IMAGE_WIDTH, IMAGE_HEIGHT\n",
        "from data.loading import load_data\n",
        "from math import ceil\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(size=(IMAGE_WIDTH, IMAGE_HEIGHT)), transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "train_ds, test_ds = load_data(transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbBdZIjn_JiO"
      },
      "source": [
        "We might not have to create a custom dataset, because we can use ImageFolder to create a dataset from an image folder with an organized structure. It will automatically associate the class names to images according to folder names.\n",
        "\n",
        "https://debuggercafe.com/pytorch-imagefolder-for-training-cnn-models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljsUITP1Dype"
      },
      "source": [
        "Images loaded are of different sizes, which would cause a\n",
        "```\n",
        "RuntimeError: stack expects each tensor to be equal size, but got [3, 3480, 4248] at entry 0 and [3, 1303, 1458] at entry 1\n",
        "\n",
        "```\n",
        "\n",
        "We can fix that by assuring all images are of the same size. Use a transform for that.\n",
        "\n",
        "https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XplHOEmvGGkZ"
      },
      "source": [
        "Apparently, we can normalize the channels to make sure brighter colors won't be of more importance. Use a transform for that.\n",
        "\n",
        "https://stats.stackexchange.com/questions/211436/why-normalize-images-by-subtracting-datasets-image-mean-instead-of-the-current"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXadZTFgSVPC"
      },
      "source": [
        "# Analyzing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "8x5YkmOqSaMa",
        "outputId": "8079a354-9a05-447a-a1e9-9a5475f50f8d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(train_ds.imgs)\n",
        "df = df.rename(columns={0: 'Img_Uri', 1: 'Class'})\n",
        "classes = df.loc[:, 'Class']\n",
        "\n",
        "plt.hist(classes)\n",
        "plt.xticks([0, 1, 2], labels=train_ds.classes)\n",
        "plt.show()\n",
        "\n",
        "df.groupby(['Class']).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ll7akpnbn7D"
      },
      "source": [
        "# Modifying Data\n",
        "\n",
        "-> Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqUqQR4qbngG",
        "outputId": "be3a636d-89bd-4496-9569-4df23e1afdbb"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "iteration = 0\n",
        "new_imgs = []\n",
        "targets = []\n",
        "samples = []\n",
        "for img in train_ds.imgs:\n",
        "  if img[1] == 0:\n",
        "    if i >= 70:\n",
        "      continue\n",
        "    i += 1\n",
        "\n",
        "  targets.append(train_ds.targets[iteration])\n",
        "  new_imgs.append(img)\n",
        "  samples.append(train_ds.samples[iteration])\n",
        "  iteration += 1\n",
        "\n",
        "train_ds.imgs = new_imgs\n",
        "train_ds.targets = targets\n",
        "train_ds.samples = samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_IMGS = len(train_ds.imgs)\n",
        "NUM_BATCHES = ceil(NUM_IMGS/setup.BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from setup import BATCH_SIZE\n",
        "\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "iterable_ds = iter(loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Declaring Components\n",
        "\n",
        "- Model\n",
        "- Optimizer\n",
        "- Loss Function\n",
        "- Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2fMaPRP0t33"
      },
      "source": [
        "## Making Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPPWB5GjHqH2"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from setup import FINE_TUNE\n",
        "from model.load_model import create_model\n",
        "\n",
        "model = create_model(fine_tune=FINE_TUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZWz0czDQNFc"
      },
      "source": [
        "## Setting Up Optimizer, Loss Function and Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6CXjKTpQP1j"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=setup.LEARNING_RATE, momentum=0.9)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Setting Up Scheduler\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=10,\n",
        "    gamma=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Jbwh5FPl_K"
      },
      "source": [
        "# Training\n",
        "\n",
        "Apparently, to run on the GPU, we have to send the data itself to the GPU. We can do so with the batches. The loss function also needs the labels to be loaded on the GPU.\n",
        "\n",
        "https://discuss.pytorch.org/t/how-to-load-all-data-into-gpu-for-training/27609/22?page=2\n",
        "\n",
        "Also, we need the weights to be on the GPU. We can do that by using the model itself, sending it to the GPU as well.\n",
        "\n",
        "https://discuss.pytorch.org/t/how-to-load-all-data-into-gpu-for-training/27609/34?page=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8w-uHjaA5w9"
      },
      "outputs": [],
      "source": [
        "# GPU or CPU Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = model.to(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_UsCHhuQFsn"
      },
      "source": [
        "Tqdm is a simple library that can be used to generate a loading bar on loops.\n",
        "\n",
        "https://github.com/tqdm/tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUZNKPmvPhzn",
        "outputId": "63d21b6e-b5e1-424c-938b-47a3537112d3"
      },
      "outputs": [],
      "source": [
        "from model.train import train\n",
        "\n",
        "train(\n",
        "    model=model,\n",
        "    num_batches=NUM_BATCHES,\n",
        "    num_epochs=setup.NUM_EPOCHS,\n",
        "    device=device,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    dataset=train_ds,\n",
        "    loss=loss,\n",
        "    num_imgs=NUM_IMGS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzhkMeogcEKT"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijprcBF46cc1"
      },
      "source": [
        "Evaluating with Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from data.evaluation import evaluate_model\n",
        "\n",
        "#evaluate_model(model=model,\n",
        "#               test_data=test_ds,\n",
        "#               batch_size=BATCH_SIZE\n",
        "#               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from data.evaluation import evaluate_single\n",
        "\n",
        "evaluate_single(model=model, classes_list=['Covid', 'Normal', 'Viral Pneumonia'], device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model.save_model import save_model\n",
        "\n",
        "save_model(model=model, optimizer=optimizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
